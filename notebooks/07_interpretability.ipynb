{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability: Attention and Attributions\n",
    "\n",
    "This notebook provides privacy-preserving interpretability: BiLSTM attention visualization and optional BERT attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from suicide_detection.models.bilstm_attention import BiLSTMAttention, BiLSTMAttentionConfig\n",
    "\n",
    "# Demo: construct a tiny vocabulary and model, then visualize attention on a sample\n",
    "vocab = {'<pad>':0, '<unk>':1, 'i':2, 'feel':3, 'okay':4, 'today':5, 'in':6, 'pain':7}\n",
    "tokens = ['i','am','in','pain']\n",
    "ids = torch.tensor([[vocab.get(t,1) for t in tokens]], dtype=torch.long)\n",
    "attn_mask = torch.ones_like(ids)\n",
    "cfg = BiLSTMAttentionConfig(vocab_size=max(vocab.values())+1, embedding_dim=16, hidden_dim=8)\n",
    "model = BiLSTMAttention(cfg)\n",
    "with torch.no_grad():\n",
    "    _ = model(ids, attn_mask)\n",
    "aw = model.last_attn.squeeze(0).numpy()\n",
    "plt.figure(figsize=(6,1.5)); plt.bar(range(len(tokens)), aw[:len(tokens)]); plt.xticks(range(len(tokens)), tokens); plt.title('BiLSTM Attention (demo)'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: BERT attributions (Integrated Gradients)\n",
    "Requires transformers-interpret and captum. Will skip gracefully if unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    from transformers_interpret import SequenceClassificationExplainer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    m = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    m.eval()\n",
    "    explainer = SequenceClassificationExplainer(m, tok)\n",
    "    txt = 'I feel okay today but sometimes I am in pain'\n",
    "    word_attributions = explainer(txt)\n",
    "    # Display top attributions (values only, no raw text snippet beyond tokens)\n",
    "    sorted_attrs = sorted([(w, float(a)) for w,a in zip(explainer.word_attributions, explainer.attributions_sum)], key=lambda x: -abs(x[1]))[:10]\n",
    "    sorted_attrs\n",
    "except Exception as e:\n",
    "    print('Attribution libraries not available or failed:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
